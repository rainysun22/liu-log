---
title: 听说你还没听说过什么是vLLM--001 原理篇
date: 2025-06-29T11:58:12+08:00
tags:
  - vllm
author: liuzifeng
---
## 到底什么是vLLM？

作为一个从没有听说过vLLM的人来说，了解一个新东西最好的方式就是打开它的文档。

点击[https://docs.vllm.ai/](https://docs.vllm.ai/)，映入眼帘的是：

![](/images/}$74YM}A5L}]1V$STCB4(PU%201.png)

你注意到了这句话：

`vLLM is a fast and easy-to-use library for LLM inference and serving.`

来自谷歌翻译：`vLLM 是一个快速且易于使用的 LLM 推理和服务库。`

下面我们从这句话入手！

### 到底什么是LLM？

全称`Large Language Model`，也就是大语言模型

诸如Deepseek、GPT、Claude这种，简单来说就是“能看懂人类语言、还能像人类一样写作和对话”的 AI 模型。

### 到底什么是推理？

简单来说就是用训练好的模型做“预测”或“生成”。

例如：

你打开Deepseek的官网，输入：`我晚上吃什么好？`

```
1.大模型睁眼一看
	它看到你说“我晚上吃什么好？”，立刻判断这是一个在询问建议的问题。
        
2.开始在脑子里翻找
	它读过成千上万次人类关于晚餐的话题，比如：“要不要吃点好的？”、“可以做个炒饭”、“点外卖也不错”。
	   
3.思考上下文
    虽然你没有明确说你是自己做、外出吃还是点外卖，它会尝试给出一个通用又贴心的建议。
        
4.组织语言
    它挑选出一个符合语境、听起来自然的回答，比如“可以吃点清淡的，比如炒青菜加鸡蛋汤“。
```

经过一会儿的头脑风暴，你就得到了回答。上面这个头脑风暴的过程就可以被称为推理。

### 还有个v呢？

别问了，我没搜到！

### 总结

总而言之，vLLM是一个加速大模型推理过程的工具。

## 推理过程在干嘛？

一个常规的LLM推理过程通常分为两个阶段**prefill和decode**，通常会使用KV cache技术加速推理。

（又听不懂了对吧，接下来听我娓娓道来..）




### prefill阶段

预填充阶段，也就是把你输入给模型的一段内容经过计算存下来。

在这个阶段，我们把用户输入的整段文本(prompt)一次性输入到模型里，让模型从头到尾对这段文本进行一次完整的计算，计算出它对每个词(token)的理解和关系。这个计算的过程称为forward计算。






